{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "XtXXbrYolf3z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWi-4XXKlbYO",
        "outputId": "d201a11e-814c-46a9-adb9-9bd25d2ee187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love my pet Elephant. Most loved animal in the world.\""
      ],
      "metadata": {
        "id": "cbuF2h7fltjz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenize\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIJA8S3nmpWX",
        "outputId": "f56e3bd2-12b4-461a-a054-49437009f223"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'my', 'pet', 'Elephant', '.', 'Most', 'loved', 'animal', 'in', 'the', 'world', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence tokenize\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKwOmHGWmuha",
        "outputId": "de1f8ef1-6dee-4d5f-cad9-c8bf4f2c39e3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I love my pet Elephant.', 'Most loved animal in the world.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words Removal"
      ],
      "metadata": {
        "id": "u4TSTfNQn9Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqVW92Ikm12T",
        "outputId": "08c388dd-6466-426a-fc78-b4c89fbf68b1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro0BnOcdoRX9",
        "outputId": "03c9d487-54af-4bec-d00d-264e6c842bd7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'pet', 'Elephant', '.', 'loved', 'animal', 'world', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "-_iz84uFp0rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "ZazdzyGIobZn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['running', 'runs', 'ran', 'run']\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VFOYXr6p8Xu",
        "outputId": "bc527a98-e6c9-4daf-f9bc-f1c03e17ae4b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'run', 'ran', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "5zypNJaSq5zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2DTB1mKp_qt",
        "outputId": "3e404118-78db-470b-f3cc-45e422094fda"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['running', 'runs', 'ran', 'run']\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc6rq4Lqq_MN",
        "outputId": "d959e0eb-020d-4d39-d843-18107ba0d9a9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'run', 'ran', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lowercasing"
      ],
      "metadata": {
        "id": "wjm3YD26rlF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_text = text.lower()\n",
        "print(lower_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvO8LHjTrERN",
        "outputId": "0f08c722-3975-4138-9375-7304d1a911ab"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love my pet elephant. most loved animal in the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punctuation Removal"
      ],
      "metadata": {
        "id": "U132ZCzItB2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "3UHoF-R0rq6T"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_no_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(text_no_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tpOe1pttGIU",
        "outputId": "f5a84726-e510-415f-e595-5c375c626407"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love my pet Elephant Most loved animal in the world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Special Characters and Numbers"
      ],
      "metadata": {
        "id": "MA2khyH4uM86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "e6z_zntetKbU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_with_special = 'User123 scored 95% in the exam!'\n",
        "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text_with_special)\n",
        "print(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uyUoQ4duSJv",
        "outputId": "f61bb3cd-0c62-4b8c-b179-a8034346befc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User scored  in the exam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spelling Correction"
      ],
      "metadata": {
        "id": "ZPa4x-PRu0Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "CBDO1KQAucOF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misspelled_text = \"Natrual Langage Processin is intresting\"\n",
        "corrected_text = str(TextBlob(misspelled_text).correct())\n",
        "print(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltlqwtmZu6A2",
        "outputId": "df028b66-974e-4296-8292-1518ec1df2e0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Procession is interesting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expanding Contractions"
      ],
      "metadata": {
        "id": "ZIXcfo1Kv4mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMkOK2fFwB-G",
        "outputId": "4a2482ef-ba1a-430a-9027-2a942e17977a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions"
      ],
      "metadata": {
        "id": "y2iVa4w_u_Rp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_with_contraction = \"I can't go because I'm tired\"\n",
        "expanded_text = contractions.fix(text_with_contraction)\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke9X685_wAY7",
        "outputId": "acff4bee-618e-4c53-e2b9-563d3ba07e4b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot go because I am tired\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing URLs, Email Address and HTML Tags"
      ],
      "metadata": {
        "id": "DhpvKcKrwn-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "PAQD5FYYwNNY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_with_html = \"<p>This is an example of <b>HTML</b> text.</p>\"\n",
        "clean_html = BeautifulSoup(text_with_html, 'html.parser').get_text()\n",
        "print(clean_html)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9emwi9INwyTK",
        "outputId": "c95986c8-3078-46c8-c775-a61497c8b6a7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of HTML text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_with_url = \"Visit https://example.com for more info.\"\n",
        "clean_url = re.sub(r'http\\S+|www.\\S+', '', text_with_url)\n",
        "print(clean_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJHBpTGkw6gE",
        "outputId": "058f29a8-32b9-4ca5-b5ce-b5c430682baa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visit  for more info.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_with_email = \"Contact me at example@email.com\"\n",
        "clean_email = re.sub(r'\\S+@\\S+', '', text_with_email)\n",
        "print(\"Text Without Email:\", clean_email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWj1dKEBxHU0",
        "outputId": "4a5d021b-b2a4-4dd3-db96-392ed571d5d8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Without Email: Contact me at \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Normalization (Slang Conversion)"
      ],
      "metadata": {
        "id": "e5eyDmi8xrzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "slang_dict = {\"u\": \"you\", \"gr8\": \"great\", \"r\": \"are\", \"b4\": \"before\"}\n",
        "text_with_slang = \"U r gr8!!\"\n",
        "\n",
        "# Convert to lowercase\n",
        "text_lower = text_with_slang.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "text_clean = text_lower.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Replace slang words\n",
        "normalized_text = ' '.join([slang_dict.get(word, word) for word in text_clean.split()])\n",
        "\n",
        "print(\"Normalized Text:\", normalized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6HTNDrLxR3s",
        "outputId": "36993fe2-1445-4834-d169-6c458a1f2586"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Text: you are great\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-of-Speech (POS) Tagging"
      ],
      "metadata": {
        "id": "BBpHz3EVy2YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4okdaWcxxZx",
        "outputId": "31465211-85af-423b-b81d-874e90076207"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"This is a sample sentence.\"\n",
        "word_tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(word_tokens)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s60pdHPLy8yl",
        "outputId": "3607ad5f-728e-4141-a743-27f9dfd625e3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k42UU-UBzkVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1:\n",
        "\n",
        "### Write a python script to lowercase text, remove punctuation and tokenize a paragraph using nltk"
      ],
      "metadata": {
        "id": "GK5pyyIWH0at"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbKWHeAOHtQq",
        "outputId": "c7ec0563-fc1b-47f2-de43-76ede26ca53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'rapid', 'advancement', 'of', 'artificial', 'intelligence', 'is', 'transforming', 'numerous', 'industries', 'from', 'healthcare', 'and', 'finance', 'to', 'transportation', 'and', 'entertainment', 'machine', 'learning', 'algorithms', 'are', 'enabling', 'computers', 'to', 'perform', 'tasks', 'once', 'thought', 'exclusive', 'to', 'humans', 'leading', 'to', 'increased', 'automation', 'personalized', 'experiences', 'and', 'novel', 'solutions', 'to', 'complex', 'problems', 'however', 'ethical', 'considerations', 'surrounding', 'ai', 'development', 'and', 'deployment', 'such', 'as', 'bias', 'in', 'algorithms', 'and', 'job', 'displacement', 'remain', 'crucial', 'areas', 'of', 'ongoing', 'discussion', 'and', 'research']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def pre_process_text(text):\n",
        "\n",
        "  # lowercase text\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove punctuation\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # tokenize text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "text = \"\"\"The rapid advancement of artificial intelligence is transforming numerous industries, from healthcare and finance to transportation and entertainment.\n",
        "Machine learning algorithms are enabling computers to perform tasks once thought exclusive to humans, leading to increased automation, personalized experiences, and novel\n",
        "solutions to complex problems.  However, ethical considerations surrounding AI development and deployment, such as bias in algorithms and job displacement, remain crucial areas\n",
        "of ongoing discussion and research.\"\"\"\n",
        "\n",
        "tokens = pre_process_text(text)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2:\n",
        "\n",
        "### Use Regex to extract emails or phone numbers from a text file"
      ],
      "metadata": {
        "id": "W8OibR7QJg_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_emails_and_phones(text):\n",
        "    # Regex for emails\n",
        "    email_pattern = r'[a-zA-Z0-9+_.-]+@[a-zA-Z0-9.-]+'\n",
        "    emails = re.findall(email_pattern, text)\n",
        "\n",
        "    # Regex for phone numbers (supports different formats)\n",
        "    phone_pattern = r'\\b\\d{10}\\b|\\(\\d{3}\\) \\d{3}-\\d{4}|\\d{3}-\\d{3}-\\d{4}'\n",
        "    phones = re.findall(phone_pattern, text)\n",
        "\n",
        "    return emails, phones\n",
        "\n",
        "# Example paragraph\n",
        "paragraph = \"Hello, world! Contact us at example@email.com or call (123) 456-7890. Another email: test123@example.org, phone: 9876543210.\"\n",
        "\n",
        "# Process the text\n",
        "emails, phones = extract_emails_and_phones(paragraph)\n",
        "\n",
        "\n",
        "print(\"Emails:\", emails)\n",
        "print(\"Phone Numbers:\", phones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns2_Z9e2JGoq",
        "outputId": "28cf1858-b8bd-4b42-ee62-e3ca068a5b29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emails: ['example@email.com', 'test123@example.org']\n",
            "Phone Numbers: ['(123) 456-7890', '9876543210']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3:\n",
        "\n",
        "### Remove stopwords from a sample text and compare output with the original"
      ],
      "metadata": {
        "id": "nTnpq0AaKBIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "filtered_tokens = remove_stopwords(tokens)\n",
        "print(\"Tokens without Stopwords:\", filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e3WE6FpJ3KE",
        "outputId": "800ae0a6-4ca7-4260-c3be-d4d8e53135dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without Stopwords: ['rapid', 'advancement', 'artificial', 'intelligence', 'transforming', 'numerous', 'industries', 'healthcare', 'finance', 'transportation', 'entertainment', 'machine', 'learning', 'algorithms', 'enabling', 'computers', 'perform', 'tasks', 'thought', 'exclusive', 'humans', 'leading', 'increased', 'automation', 'personalized', 'experiences', 'novel', 'solutions', 'complex', 'problems', 'however', 'ethical', 'considerations', 'surrounding', 'ai', 'development', 'deployment', 'bias', 'algorithms', 'job', 'displacement', 'remain', 'crucial', 'areas', 'ongoing', 'discussion', 'research']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3r1ogP7JKdWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}